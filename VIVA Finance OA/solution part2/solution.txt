To create an efficient database for this situation, I would use a relational database such as MySQL. For the primary key, I would set it to the username since the username value is unique and each username corresponds to a set of attributes such as first name, last name,  and employer. My relational database would be made up of two tables, one table with the primary key that associates usernames to the name of the customer, their company, and their mask. My second database would use the username as the primary key again, but this table has amount paid and amount owed as the new attributes. Now, whenever I receive a new query, I can do a quick lookup in my first database to find the customer id of this customer (if he is a new customer, I simply add him to the database and assign him an unique username) and after finding his username I can update my second database with the amount paid and amount owed. My previous solution required O(n^2) time to complete since for every element in my json file, I had to traverse the csv file to find corresponding values. With this new database, I no longer have to search every query, instead I can do a O(n) lookup in my first database and another O(n) update in my second database. Therefore, the time complexity will reduce to just O(n). To further optimize my database, I can define custom indices to speed up query lookup times. An example of this would be assigning customers who do more frequent transactions as having higher indices. If there is a lot of queries, the databases might overload so using replication techniques such as master-slave databases will help reduce load. We can allow the master database to only be written to, and use the slave databases which are copies of the master database to read from. 